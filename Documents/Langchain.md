## What is Langchain?
==Langchain is a framework for interacting with LLMs. LLMs are large language models which are a type of artificial intelligence that can understand and generate human-like text.== It’s trained on massive amounts of text data to learn patterns, language structures, and context, enabling it to answer questions, write stories, translate languages, and much more. Some LLMs are ChatGPT, Mistral, and Phi. LangChain's comprehensive design enables three main solutions: summarization/query engine, chatbot, and knowledge agent.

## What are LLMs used for?
==LLMs are excellent for tasks involving natural language understanding, text generation, and processing. They are used in text summarization, translation, sentiment analysis, search engines, chatbots, and code generation.== Due to these capabilities, LLMs are increasingly popular in fields like education, healthcare, legal work, and financial analysis. Most LLM-based applications follow common workflows and share technical foundations. They handle natural language requests, process unstructured data, import data from various sources, and integrate user prompts with LLMs. LLM applications can be grouped into three main patterns: LLM-based engines, LLM-based chatbots, and LLM-based autonomous agents. 

## What are  LLM-based engines?
==An LLM-based engine acts as a backend tool for different computer systems, handling specific requests to an LLM.== For example, it can serve as a summarization engine, condensing text passages into concise summaries.  Alternatively, an LLM-based engine can handle semantic search or Question & Answer task, providing responses to queries made against a knowledge base. This Q&A engine operates in two phases: the text ingestion phase and the Q&A phase.
During the text ingestion phase, the engine builds the knowledge base by importing and organizing text content for efficient retrieval into a vector store using embedding models. Don't worry about terms like "vector store" and "embedding model"; I will cover them extensively in the book. For now, just understand that during this phase, the Q&A engine transforms text into vectors that capture the underlying meaning, allowing for efficient storage and retrieval. During the Q&A phase, the engine performs a semantic search on the vector store to retrieve relevant text chunks based on the question's vector representation. These chunks serve as the question's "context" and are combined with the original question or "instruction" into a "prompt" fed to the LLM to generate a "completion." This method is known as Retrieval Augmented Generation (RAG). Retrieval Augmented Generation (RAG) is a design or architecture for LLM applications where the LLM's completion “generation” process is “augmented” by incorporating a context enriched through the “retrieval” of pertinent facts from a local knowledge base, often stored in a vector store. ==Essentially, LLM-based engines primarily support system-level interactions, emphasizing automation, data processing, and integration with external systems to simplify complex workflows involving Natural Language Processing (NLP) and improve information retrieval and processing efficiency.== When the user of the LLM application isn't an external system but rather a person seeking to ask a question and refine the answer interactively, the appropriate solution is an LLM-based chatbot. ![[ch01__image009.png]]
## What are LLM-based Chatbots?
==An LLM chatbot is like a smart assistant that enables interactive conversations with the language model.== It doesn't just facilitate communication; it makes it more efficient and safer. It does this by using well-engineered prompts with additional instructions to guide the LLM to better results or constrain it to produce safer outputs. The chatbot can also augment the prompt with facts retrieved from a local knowledge base. It remembers what you've said and engages in dynamic chats with you, creating a bridge between you and the LLM. Different types of chatbots are specialized in these common tasks: summarizing text, answering questions, and translating languages. A chatbot helps you accomplish your tasks by using the knowledge it already has (for example located in a vector store) or by taking input from you through its user interface.

## What are LLM-based Autonomous Agents?
==An LLM-based autonomous agent is a sophisticated tool built to handle complex workflows in collaboration with the LLM, especially when dealing with diverse data sources and branching workflows.== These autonomous agents are designed to perform complex tasks by connecting to various structured and unstructured data sources. They demonstrate a high degree of independence at each step of the workflow when interacting with the LLM, ultimately delivering comprehensive results.
Figure 1: ![[ch01__image017.png]]
## What is Langchain's Architecture?
==Langchain's Architecture works as this: Text is gathered from various sources like files, databases, and websites, then parsed into Document objects or smaller chunks. These are processed by an embedding model to generate vector representations that capture semantic meaning. The original chunks and their embeddings are stored in a vector form. When an LLM application requests a task like summarizing text or conducting a semantic search, it sends a prompt with the original question and a "context" made up of a Document or relevant text chunks retrieved from the vector store==. In some cases, information may also be retrieved from a graph database. A detailed description of each component is (ordered by when they are used in the process from start to finish):
- Document loaders: in LangChain, document loaders play a pivotal role by extracting data from various sources and transforming it into `Document` objects. These `Document` objects serve as the fundamental entities processed within the system. Think of Document loaders as the translator for the rest of the system to be able to understand the data that it is taking in.
- Text Splitters: Text Splitters are crucial for breaking down text from a data source into smaller `Document` instances or "chunks." This approach helps overcome the limitation of the maximum prompt sizer, or context windows. It's especially relevant in the ingestion phase of the Question & Answer use case, where the original text is split before being indexed with embeddings and stored in the vector database. Think of Text Splitters as the little trash compacters making the data nice and small so that the embedding model can ingest it nice, easy, and fast.
- `Document`: A `Document` is the core entity in Langchain, representing both the content and metadata of structured or unstructered text. For example, text from a PDF can be instantiated as a `Document` object, with the source file name. A text splitter can break it into a list of `Document` objects.
- Embedding models: these models are designed to capture the semantic meaning of text and convert it into numerical vectors. These numerical vectors are something the LLM can understand and process. Embeddings are vector representations of words or tokens in a continuous, high-dimensional space (typically between 1500 and 2500 dimensions). They capture both semantic and syntactic information, allowing the LLM to recognize relationships between words. Embeddings are learned during the pre-training phase, where the model is trained on large text datasets to predict tokens in context.
- Vector Stores: Vectpr stores operate as specialized databases designed for the efficent retrieval of `Document` objects. these objects typically represent fragments or "chunks" of the original object, indexed based on their associated embeddings. This indexing allows for search queries to match the embeddings of the chunks against those of the query. Basically the whole process turns the files into document objects, to smaller bite size fragments and then into numbers that the LLM can process and understand. By ingesting `Documents` along with their associated embeddings, the vector store can serve as an offline knowledge base for proprietary data.
-  Knowledge Graph databases: Although not a key component of the architecture, LangChain offers client wrappers for leading graph databases to facilitate Knowledge Graph functionality. These databases store entities and their relationships in a graph form.
- Retrievers: In LangChain, retrievers efficiently fetch data, often a list of Documents containing unstructured text, from indexed databases like vector stores. LangChain provides support for various retrievers, catering not only to vector stores but also to relational databases and more complex data stores, such as knowledge graph databases like Neo4j.
- Prompts: LangChain offers functionality to model prompt templates and convert them into specific prompt instances. These instances include user input or text from offline sources, such as a vector store. Once created, the entire prompt instance undergoes processing by the Language Model (LLM). To enhance prompts, examples can be added.
- LLM cache: An optional component that improves performance and reduces costs by minimizing calls to the LLM. It provides immediate answers to questions previously asked. Imagine "Hey man you already asked that so I don't even have to go check."
- LLM / ChatModel: This component acts as an interface to an LLM or chat model built on an LLM. LangChain supports multiple LLMs and associated chat models, including those from OpenAI, Cohere, and HuggingFace. Think of it like the stuff you see when you are chatting with a model like ChatGPT. 
- Output Parser: This component transforms an LLM's natural language responce into a structured format, such as JSON, making it easier for an application using LLMs to process.
These components listed above can be organized into a chain or structured around agents.
See Figure 1 for a visual look at Langchain's architecture.

Figure 2: ![[ch01__image001.png]]


## What are Chains and Agents?
A chain is a composite arrangement guiding LangChain's processing workflow, customized for specfic use cases and based on a sequence of the described components. A chain is a pipeline of components put together to achieve a particular outcome. Example: `chain = web_scraping | prompt | llm_model | email_text` An agent is a component that manages a dynamic workflow, extending a sequential chain. The agent's processing is flexible and can adapt based on user input or component output. Resources in the dynamic workflow are called "tools" or "plugins" in most frameworks' documentation and the collection of all tools is referred to as the "toolkit."

## How is LangChain's object model organized?
The object model is built on different class hierarchies, starting with a base abstract class from which several concrete classes are derived. These clases include Document, DocumentLoader, TextSplitter, VectorStore and Retriever. 
Figure 3: ![[ch01__image005.png]]
The object model for Language Models visual:
Figure 4:![[ch01__image007.png]]
## How do I write prompts programmatically?
Prompt engineering -- designing and refining prompts to guide the LLM's output -- is a core skill in developing any LLM-based application. Here are some patterns for making prompts for different LLM apps. Text Classification is split into 3 different categories: the instructions, the input, and the output. Be sure to keep the output to just the category the text is in. Next is sentiment analysis which is split into the instruction, text and the output.  Next, text summarization only uses the instruction of how long you want the summary to be and the text that you are summarizing.  ==Remember: creating a well-structured prompt with defined sections helps the LLM understand your requirements better and generate the excepted text.== When developing LLM applications its important to understand the limitations of different prompt techniques. For instance, "Zero-shot training" is a method where you provide zero examples, and except the ai to grasp and perform the task with only the instructions given. "One-shot training" is a method where you give the A.I. one example. This continues into 2,3,4,ect shot training scaling with the amount of examples. Another method is trying to explain the way it should think in both examples and the logical steps to achieve those examples. This is called "chain of thought".